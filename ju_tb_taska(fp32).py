# -*- coding: utf-8 -*-
"""JU-TB-TaskA(FP32).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sdan92O2Z7RKpOY_0MkVneMPoQLKU2uS
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/data/final_dataset.zip"

"""## MobileNetV2"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

data_dir = "/content/final_dataset"  # change if your dataset is elsewhere
train_dir = os.path.join(data_dir, "train")
val_dir   = os.path.join(data_dir, "val")
test_dir  = os.path.join(data_dir, "test")

IMG_SIZE = (224, 224)
BATCH_SIZE = 2

train_ds = image_dataset_from_directory(
    train_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

val_ds = image_dataset_from_directory(
    val_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

test_ds = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

class_names = train_ds.class_names
print("Classes:", class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds   = val_ds.prefetch(AUTOTUNE)
test_ds  = test_ds.prefetch(AUTOTUNE)

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))
base_model.trainable = False

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(2, activation='softmax')  # binary classification (normal vs tb)
])

class F1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=2, name='f1_score', dtype=tf.float32, **kwargs):
        super().__init__(name=name, dtype=dtype, **kwargs)
        self.num_classes = int(num_classes)  # must be a Python int (compile-time constant)
        # accumulators for true positives, false positives, false negatives per class
        self.tp = self.add_weight(name='tp', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)
        self.fp = self.add_weight(name='fp', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)
        self.fn = self.add_weight(name='fn', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)

    def update_state(self, y_true, y_pred, sample_weight=None):
        """
        y_true: one-hot or sparse; y_pred: logits or probs
        We convert both to class indices and update TP/FP/FN.
        """
        # if one-hot -> convert to indices
        if y_true.shape.ndims == 2:
            y_true_idx = tf.argmax(y_true, axis=1, output_type=tf.int32)
        else:
            y_true_idx = tf.cast(tf.reshape(y_true, [-1]), tf.int32)

        # predicted class indices
        if y_pred.shape.ndims == 2:
            y_pred_idx = tf.argmax(y_pred, axis=1, output_type=tf.int32)
        else:
            y_pred_idx = tf.cast(tf.reshape(y_pred, [-1]), tf.int32)

        # confusion matrix for this batch with fixed num_classes
        cm = tf.math.confusion_matrix(y_true_idx, y_pred_idx,
                                      num_classes=self.num_classes, dtype=tf.float32)

        # diagonal are TP per class
        batch_tp = tf.linalg.diag_part(cm)
        batch_fp = tf.reduce_sum(cm, axis=0) - batch_tp
        batch_fn = tf.reduce_sum(cm, axis=1) - batch_tp

        # update accumulators
        self.tp.assign_add(batch_tp)
        self.fp.assign_add(batch_fp)
        self.fn.assign_add(batch_fn)

    def result(self):
        precision = tf.math.divide_no_nan(self.tp, self.tp + self.fp)
        recall = tf.math.divide_no_nan(self.tp, self.tp + self.fn)
        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)
        # return macro F1 (mean over classes)
        return tf.reduce_mean(f1)

    def reset_states(self):
        for v in (self.tp, self.fp, self.fn):
            v.assign(tf.zeros_like(v))

precision = tf.keras.metrics.Precision(name='precision')
recall = tf.keras.metrics.Recall(name='recall')

f1_metric = F1Score(num_classes=2, name='f1_score')

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy', precision, recall, f1_metric]
)

model.summary()

callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    ModelCheckpoint('mobilenetv2_tb_best.h5', monitor='val_accuracy', save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=1)
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks
)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend(); plt.title("Accuracy")

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend(); plt.title("Loss")
plt.show()

print("\nEvaluating on Test Set ...")
test_loss, test_acc, test_prec, test_rec, test_f1 = model.evaluate(test_ds)
print(f"Test Accuracy: {test_acc*100:.2f}%")
print(f"Precision: {test_prec*100:.2f}%")
print(f"Recall: {test_rec*100:.2f}%")
print(f"F1-score: {test_f1*100:.2f}%")

y_true = []
y_pred = []

for images, labels in test_ds:
    preds = model.predict(images,verbose=0)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=class_names, digits=4))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

raw_test = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=1,
    label_mode='categorical',
    shuffle=False
)

normal_imgs = []
tb_imgs = []

for img, label in raw_test:
    cls = tf.argmax(label[0]).numpy()
    if cls == 0 and len(normal_imgs) < 16:
        normal_imgs.append((img[0].numpy(), 0))
    if cls == 1 and len(tb_imgs) < 16:
        tb_imgs.append((img[0].numpy(), 1))
    if len(normal_imgs) == 16 and len(tb_imgs) == 16:
        break

selected = normal_imgs + tb_imgs

import cv2

def make_gradcam_heatmap(img_array, model, last_conv_layer_name):
    # Get base model
    base_model = model.layers[0]
    last_conv_layer = base_model.get_layer(last_conv_layer_name)

    # Create gradient model
    grad_model = tf.keras.Model(
        inputs=base_model.input,
        outputs=[last_conv_layer.output, base_model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, base_output = grad_model(img_array)

        # Pass through rest of model
        x = base_output
        for layer in model.layers[1:]:
            x = layer(x)

        pred_index = tf.argmax(x[0])
        class_channel = x[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)
    return heatmap.numpy()

def overlay_heatmap(img, heatmap, alpha=0.4):
    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap_uint8 = np.uint8(255 * heatmap_resized)
    heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)
    img_uint8 = img.astype(np.uint8)
    superimposed = cv2.addWeighted(img_uint8, 1 - alpha, heatmap_colored, alpha, 0)
    return superimposed

for idx, (orig, label_idx) in enumerate(selected):
    print(f"Processing image {idx+1}/{len(selected)} - Class: {class_names[label_idx]}")

    # Normalize original image for display (0-255 range)
    img_display = orig.copy()
    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())
    img_display = (img_display * 255).astype(np.uint8)

    img_preprocessed = preprocess_input(np.expand_dims(orig, 0))
    heatmap = make_gradcam_heatmap(img_preprocessed, model, "Conv_1")
    cam_overlay = overlay_heatmap(img_display, heatmap, alpha=0.4)

    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1)
    plt.imshow(img_display)
    plt.title(f"Original\nClass: {class_names[label_idx]}", fontsize=12, weight='bold')
    plt.axis("off")

    # plt.subplot(1, 3, 2)
    # plt.imshow(heatmap, cmap='jet')
    # plt.title("Grad-CAM Heatmap", fontsize=12, weight='bold')
    # plt.colorbar(fraction=0.046, pad=0.04)
    # plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(cam_overlay)
    plt.title("Grad-CAM Overlay", fontsize=12, weight='bold')
    plt.axis("off")

    plt.tight_layout()
    plt.show()

"""## EfficientNetB0"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

data_dir = "/content/final_dataset"
train_dir = os.path.join(data_dir, "train")
val_dir   = os.path.join(data_dir, "val")
test_dir  = os.path.join(data_dir, "test")

IMG_SIZE = (224, 224)
BATCH_SIZE = 2

train_ds = image_dataset_from_directory(
    train_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

val_ds = image_dataset_from_directory(
    val_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

test_ds = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

class_names = train_ds.class_names
print("Classes:", class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds   = val_ds.prefetch(AUTOTUNE)
test_ds  = test_ds.prefetch(AUTOTUNE)

base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))
base_model.trainable = False

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(2, activation='softmax')
])

class F1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=2, name='f1_score', dtype=tf.float32, **kwargs):
        super().__init__(name=name, dtype=dtype, **kwargs)
        self.num_classes = int(num_classes)
        self.tp = self.add_weight(name='tp', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)
        self.fp = self.add_weight(name='fp', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)
        self.fn = self.add_weight(name='fn', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)

    def update_state(self, y_true, y_pred, sample_weight=None):
        if y_true.shape.ndims == 2:
            y_true_idx = tf.argmax(y_true, axis=1, output_type=tf.int32)
        else:
            y_true_idx = tf.cast(tf.reshape(y_true, [-1]), tf.int32)

        if y_pred.shape.ndims == 2:
            y_pred_idx = tf.argmax(y_pred, axis=1, output_type=tf.int32)
        else:
            y_pred_idx = tf.cast(tf.reshape(y_pred, [-1]), tf.int32)

        cm = tf.math.confusion_matrix(y_true_idx, y_pred_idx,
                                      num_classes=self.num_classes, dtype=tf.float32)

        batch_tp = tf.linalg.diag_part(cm)
        batch_fp = tf.reduce_sum(cm, axis=0) - batch_tp
        batch_fn = tf.reduce_sum(cm, axis=1) - batch_tp

        self.tp.assign_add(batch_tp)
        self.fp.assign_add(batch_fp)
        self.fn.assign_add(batch_fn)

    def result(self):
        precision = tf.math.divide_no_nan(self.tp, self.tp + self.fp)
        recall = tf.math.divide_no_nan(self.tp, self.tp + self.fn)
        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)
        return tf.reduce_mean(f1)

    def reset_states(self):
        for v in (self.tp, self.fp, self.fn):
            v.assign(tf.zeros_like(v))

precision = tf.keras.metrics.Precision(name='precision')
recall = tf.keras.metrics.Recall(name='recall')
f1_metric = F1Score(num_classes=2, name='f1_score')

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy', precision, recall, f1_metric]
)

model.summary()

callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    ModelCheckpoint('efficientnetb0_tb_best.h5', monitor='val_accuracy', save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=1)
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks
)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend(); plt.title("Accuracy")

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend(); plt.title("Loss")
plt.show()

print("\nEvaluating on Test Set ...")
test_loss, test_acc, test_prec, test_rec, test_f1 = model.evaluate(test_ds)
print(f"Test Accuracy: {test_acc*100:.2f}%")
print(f"Precision: {test_prec*100:.2f}%")
print(f"Recall: {test_rec*100:.2f}%")
print(f"F1-score: {test_f1*100:.2f}%")

y_true = []
y_pred = []

for images, labels in test_ds:
    preds = model.predict(images,verbose=0)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=class_names, digits=4))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

raw_test = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=1,
    label_mode='categorical',
    shuffle=False
)

normal_imgs = []
tb_imgs = []

for img, label in raw_test:
    cls = tf.argmax(label[0]).numpy()
    if cls == 0 and len(normal_imgs) < 16:
        normal_imgs.append((img[0].numpy(), 0))
    if cls == 1 and len(tb_imgs) < 16:
        tb_imgs.append((img[0].numpy(), 1))
    if len(normal_imgs) == 16 and len(tb_imgs) == 16:
        break

selected = normal_imgs + tb_imgs

import cv2

def make_gradcam_heatmap(img_array, model, last_conv_layer_name):
    # Get base model
    base_model = model.layers[0]
    last_conv_layer = base_model.get_layer(last_conv_layer_name)

    # Create gradient model
    grad_model = tf.keras.Model(
        inputs=base_model.input,
        outputs=[last_conv_layer.output, base_model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, base_output = grad_model(img_array)

        # Pass through rest of model
        x = base_output
        for layer in model.layers[1:]:
            x = layer(x)

        pred_index = tf.argmax(x[0])
        class_channel = x[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)
    return heatmap.numpy()

def overlay_heatmap(img, heatmap, alpha=0.4):
    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap_uint8 = np.uint8(255 * heatmap_resized)
    heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)
    img_uint8 = img.astype(np.uint8)
    superimposed = cv2.addWeighted(img_uint8, 1 - alpha, heatmap_colored, alpha, 0)
    return superimposed

for idx, (orig, label_idx) in enumerate(selected):
    print(f"Processing image {idx+1}/{len(selected)} - Class: {class_names[label_idx]}")

    # Normalize original image for display (0-255 range)
    img_display = orig.copy()
    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())
    img_display = (img_display * 255).astype(np.uint8)

    img_preprocessed = preprocess_input(np.expand_dims(orig, 0))
    heatmap = make_gradcam_heatmap(img_preprocessed, model, "block7a_project_conv") # Changed from "Conv_1"
    cam_overlay = overlay_heatmap(img_display, heatmap, alpha=0.4)

    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1)
    plt.imshow(img_display)
    plt.title(f"Original\nClass: {class_names[label_idx]}", fontsize=12, weight='bold')
    plt.axis("off")

    # plt.subplot(1, 3, 2)
    # plt.imshow(heatmap, cmap='jet')
    # plt.title("Grad-CAM Heatmap", fontsize=12, weight='bold')
    # plt.colorbar(fraction=0.046, pad=0.04)
    # plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(cam_overlay)
    plt.title("Grad-CAM Overlay", fontsize=12, weight='bold')
    plt.axis("off")

    plt.tight_layout()
    plt.show()

"""## ResNet50"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet import preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

data_dir = "/content/final_dataset"
train_dir = os.path.join(data_dir, "train")
val_dir   = os.path.join(data_dir, "val")
test_dir  = os.path.join(data_dir, "test")

IMG_SIZE = (224, 224)
BATCH_SIZE = 2

train_ds = image_dataset_from_directory(
    train_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

val_ds = image_dataset_from_directory(
    val_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

test_ds = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

class_names = train_ds.class_names
print("Classes:", class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds   = val_ds.prefetch(AUTOTUNE)
test_ds  = test_ds.prefetch(AUTOTUNE)

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))
base_model.trainable = False

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(2, activation='softmax')
])

class F1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=2, name='f1_score', dtype=tf.float32, **kwargs):
        super().__init__(name=name, dtype=dtype, **kwargs)
        self.num_classes = int(num_classes)
        self.tp = self.add_weight(name='tp', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)
        self.fp = self.add_weight(name='fp', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)
        self.fn = self.add_weight(name='fn', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)

    def update_state(self, y_true, y_pred, sample_weight=None):
        if y_true.shape.ndims == 2:
            y_true_idx = tf.argmax(y_true, axis=1, output_type=tf.int32)
        else:
            y_true_idx = tf.cast(tf.reshape(y_true, [-1]), tf.int32)

        if y_pred.shape.ndims == 2:
            y_pred_idx = tf.argmax(y_pred, axis=1, output_type=tf.int32)
        else:
            y_pred_idx = tf.cast(tf.reshape(y_pred, [-1]), tf.int32)

        cm = tf.math.confusion_matrix(y_true_idx, y_pred_idx,
                                      num_classes=self.num_classes, dtype=tf.float32)

        batch_tp = tf.linalg.diag_part(cm)
        batch_fp = tf.reduce_sum(cm, axis=0) - batch_tp
        batch_fn = tf.reduce_sum(cm, axis=1) - batch_tp

        self.tp.assign_add(batch_tp)
        self.fp.assign_add(batch_fp)
        self.fn.assign_add(batch_fn)

    def result(self):
        precision = tf.math.divide_no_nan(self.tp, self.tp + self.fp)
        recall = tf.math.divide_no_nan(self.tp, self.tp + self.fn)
        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)
        return tf.reduce_mean(f1)

    def reset_states(self):
        for v in (self.tp, self.fp, self.fn):
            v.assign(tf.zeros_like(v))

precision = tf.keras.metrics.Precision(name='precision')
recall = tf.keras.metrics.Recall(name='recall')
f1_metric = F1Score(num_classes=2, name='f1_score')

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy', precision, recall, f1_metric]
)

model.summary()

callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    ModelCheckpoint('resnet50_tb_best.h5', monitor='val_accuracy', save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=1)
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks
)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend(); plt.title("Accuracy")

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend(); plt.title("Loss")
plt.show()

print("\nEvaluating on Test Set ...")
test_loss, test_acc, test_prec, test_rec, test_f1 = model.evaluate(test_ds)
print(f"Test Accuracy: {test_acc*100:.2f}%")
print(f"Precision: {test_prec*100:.2f}%")
print(f"Recall: {test_rec*100:.2f}%")
print(f"F1-score: {test_f1*100:.2f}%")

y_true = []
y_pred = []

for images, labels in test_ds:
    preds = model.predict(images,verbose=0)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=class_names, digits=4))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

raw_test = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=1,
    label_mode='categorical',
    shuffle=False
)

normal_imgs = []
tb_imgs = []

for img, label in raw_test:
    cls = tf.argmax(label[0]).numpy()
    if cls == 0 and len(normal_imgs) < 16:
        normal_imgs.append((img[0].numpy(), 0))
    if cls == 1 and len(tb_imgs) < 16:
        tb_imgs.append((img[0].numpy(), 1))
    if len(normal_imgs) == 16 and len(tb_imgs) == 16:
        break

selected = normal_imgs + tb_imgs

import cv2

def make_gradcam_heatmap(img_array, model, last_conv_layer_name):
    # Get base model
    base_model = model.layers[0]
    last_conv_layer = base_model.get_layer(last_conv_layer_name)

    # Create gradient model
    grad_model = tf.keras.Model(
        inputs=base_model.input,
        outputs=[last_conv_layer.output, base_model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, base_output = grad_model(img_array)

        # Pass through rest of model
        x = base_output
        for layer in model.layers[1:]:
            x = layer(x)

        pred_index = tf.argmax(x[0])
        class_channel = x[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)
    return heatmap.numpy()

def overlay_heatmap(img, heatmap, alpha=0.4):
    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap_uint8 = np.uint8(255 * heatmap_resized)
    heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)
    img_uint8 = img.astype(np.uint8)
    superimposed = cv2.addWeighted(img_uint8, 1 - alpha, heatmap_colored, alpha, 0)
    return superimposed

for idx, (orig, label_idx) in enumerate(selected):
    print(f"Processing image {idx+1}/{len(selected)} - Class: {class_names[label_idx]}")

    # Normalize original image for display (0-255 range)
    img_display = orig.copy()
    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())
    img_display = (img_display * 255).astype(np.uint8)

    img_preprocessed = preprocess_input(np.expand_dims(orig, 0))
    heatmap = make_gradcam_heatmap(img_preprocessed, model, "conv5_block3_out") # Changed from "block7a_project_conv"
    cam_overlay = overlay_heatmap(img_display, heatmap, alpha=0.4)

    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1)
    plt.imshow(img_display)
    plt.title(f"Original\nClass: {class_names[label_idx]}", fontsize=12, weight='bold')
    plt.axis("off")

    # plt.subplot(1, 3, 2)
    # plt.imshow(heatmap, cmap='jet')
    # plt.title("Grad-CAM Heatmap", fontsize=12, weight='bold')
    # plt.colorbar(fraction=0.046, pad=0.04)
    # plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(cam_overlay)
    plt.title("Grad-CAM Overlay", fontsize=12, weight='bold')
    plt.axis("off")

    plt.tight_layout()
    plt.show()

"""## InceptionNetV3"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

data_dir = "/content/final_dataset"
train_dir = os.path.join(data_dir, "train")
val_dir   = os.path.join(data_dir, "val")
test_dir  = os.path.join(data_dir, "test")

IMG_SIZE = (224, 224)
BATCH_SIZE = 2

train_ds = image_dataset_from_directory(
    train_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

val_ds = image_dataset_from_directory(
    val_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

test_ds = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

class_names = train_ds.class_names
print("Classes:", class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds   = val_ds.prefetch(AUTOTUNE)
test_ds  = test_ds.prefetch(AUTOTUNE)

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3))
base_model.trainable = False

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(2, activation='softmax')
])

class F1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=2, name='f1_score', dtype=tf.float32, **kwargs):
        super().__init__(name=name, dtype=dtype, **kwargs)
        self.num_classes = int(num_classes)
        self.tp = self.add_weight(name='tp', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)
        self.fp = self.add_weight(name='fp', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)
        self.fn = self.add_weight(name='fn', shape=(self.num_classes,), initializer='zeros', dtype=tf.float32)

    def update_state(self, y_true, y_pred, sample_weight=None):
        if y_true.shape.ndims == 2:
            y_true_idx = tf.argmax(y_true, axis=1, output_type=tf.int32)
        else:
            y_true_idx = tf.cast(tf.reshape(y_true, [-1]), tf.int32)

        if y_pred.shape.ndims == 2:
            y_pred_idx = tf.argmax(y_pred, axis=1, output_type=tf.int32)
        else:
            y_pred_idx = tf.cast(tf.reshape(y_pred, [-1]), tf.int32)

        cm = tf.math.confusion_matrix(y_true_idx, y_pred_idx,
                                      num_classes=self.num_classes, dtype=tf.float32)

        batch_tp = tf.linalg.diag_part(cm)
        batch_fp = tf.reduce_sum(cm, axis=0) - batch_tp
        batch_fn = tf.reduce_sum(cm, axis=1) - batch_tp

        self.tp.assign_add(batch_tp)
        self.fp.assign_add(batch_fp)
        self.fn.assign_add(batch_fn)

    def result(self):
        precision = tf.math.divide_no_nan(self.tp, self.tp + self.fp)
        recall = tf.math.divide_no_nan(self.tp, self.tp + self.fn)
        f1 = tf.math.divide_no_nan(2 * precision * recall, precision + recall)
        return tf.reduce_mean(f1)

    def reset_states(self):
        for v in (self.tp, self.fp, self.fn):
            v.assign(tf.zeros_like(v))

precision = tf.keras.metrics.Precision(name='precision')
recall = tf.keras.metrics.Recall(name='recall')
f1_metric = F1Score(num_classes=2, name='f1_score')

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy', precision, recall, f1_metric]
)

model.summary()

callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    ModelCheckpoint('inceptionv3_tb_best.h5', monitor='val_accuracy', save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=1)
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks
)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend(); plt.title("Accuracy")

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend(); plt.title("Loss")
plt.show()

print("\nEvaluating on Test Set ...")
test_loss, test_acc, test_prec, test_rec, test_f1 = model.evaluate(test_ds)
print(f"Test Accuracy: {test_acc*100:.2f}%")
print(f"Precision: {test_prec*100:.2f}%")
print(f"Recall: {test_rec*100:.2f}%")
print(f"F1-score: {test_f1*100:.2f}%")

y_true = []
y_pred = []

for images, labels in test_ds:
    preds = model.predict(images,verbose=0)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=class_names, digits=4))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

raw_test = image_dataset_from_directory(
    test_dir,
    image_size=IMG_SIZE,
    batch_size=1,
    label_mode='categorical',
    shuffle=False
)

normal_imgs = []
tb_imgs = []

for img, label in raw_test:
    cls = tf.argmax(label[0]).numpy()
    if cls == 0 and len(normal_imgs) < 16:
        normal_imgs.append((img[0].numpy(), 0))
    if cls == 1 and len(tb_imgs) < 16:
        tb_imgs.append((img[0].numpy(), 1))
    if len(normal_imgs) == 16 and len(tb_imgs) == 16:
        break

selected = normal_imgs + tb_imgs

import cv2

def make_gradcam_heatmap(img_array, model, last_conv_layer_name):
    # Get base model
    base_model = model.layers[0]
    last_conv_layer = base_model.get_layer(last_conv_layer_name)

    # Create gradient model
    grad_model = tf.keras.Model(
        inputs=base_model.input,
        outputs=[last_conv_layer.output, base_model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, base_output = grad_model(img_array)

        # Pass through rest of model
        x = base_output
        for layer in model.layers[1:]:
            x = layer(x)

        pred_index = tf.argmax(x[0])
        class_channel = x[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)
    return heatmap.numpy()

def overlay_heatmap(img, heatmap, alpha=0.4):
    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap_uint8 = np.uint8(255 * heatmap_resized)
    heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)
    img_uint8 = img.astype(np.uint8)
    superimposed = cv2.addWeighted(img_uint8, 1 - alpha, heatmap_colored, alpha, 0)
    return superimposed

for idx, (orig, label_idx) in enumerate(selected):
    print(f"Processing image {idx+1}/{len(selected)} - Class: {class_names[label_idx]}")

    # Normalize original image for display (0-255 range)
    img_display = orig.copy()
    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())
    img_display = (img_display * 255).astype(np.uint8)

    img_preprocessed = preprocess_input(np.expand_dims(orig, 0))
    heatmap = make_gradcam_heatmap(img_preprocessed, model, "conv2d_93") # Changed from "block7a_project_conv"
    cam_overlay = overlay_heatmap(img_display, heatmap, alpha=0.4)

    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1)
    plt.imshow(img_display)
    plt.title(f"Original\nClass: {class_names[label_idx]}", fontsize=12, weight='bold')
    plt.axis("off")

    # plt.subplot(1, 3, 2)
    # plt.imshow(heatmap, cmap='jet')
    # plt.title("Grad-CAM Heatmap", fontsize=12, weight='bold')
    # plt.colorbar(fraction=0.046, pad=0.04)
    # plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(cam_overlay)
    plt.title("Grad-CAM Overlay", fontsize=12, weight='bold')
    plt.axis("off")

    plt.tight_layout()
    plt.show()

